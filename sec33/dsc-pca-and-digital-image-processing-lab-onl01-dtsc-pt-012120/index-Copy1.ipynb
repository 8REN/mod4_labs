{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Recognition with PCA - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll explore the classic MNIST dataset of handwritten digits. While not as large as the previous dataset on facial image recognition, it still provides a 64-dimensional dataset that is ripe for feature reduction.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab you will: \n",
    "\n",
    "- Use PCA to discover the principal components with images \n",
    "- Use the principal components of  a dataset as features in a machine learning model \n",
    "- Calculate the time savings and performance gains of layering in PCA as a preprocessing step in machine learning pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Load the `load_digits` dataset from the `datasets` module of scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from sklearn.datasets import load_digits\n",
    "data = load_digits()\n",
    "print(data.data.shape, data.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview the dataset\n",
    "\n",
    "Now that the dataset is loaded, display the first 20 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(10,10))\n",
    "for n in range(20):\n",
    "    i = n //5\n",
    "    j = n%5\n",
    "    ax = axes[i][j]\n",
    "    ax.imshow(data.images[n], cmap=plt.cm.gray)\n",
    "plt.title('First 20 Images From the MNIST Dataset');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "Now it's time to fit an initial baseline model. \n",
    "\n",
    "- Split the data into training and test sets. Set `random_state=22` \n",
    "- Fit a support vector machine to the dataset. Set `gamma='auto'` \n",
    "- Record the training time \n",
    "- Print the training and test accucary of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=22)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a naive model \n",
    "clf = svm.SVC(gamma='auto')\n",
    "%timeit clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test accuracy\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search baseline\n",
    "\n",
    "Refine the initial model by performing a grid search to tune the hyperparameters. The two most important parameters to adjust are `'C'` and `'gamma'`. Once again, be sure to record the training time as well as the training and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = svm.SVC()\n",
    "\n",
    "param_grid = {'C' : np.linspace(.1, 10, num=11),\n",
    "             'gamma' : np.linspace(10**-3, 5, num=11)}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "%timeit grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters \n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the training and test accuracy \n",
    "train_acc = grid_search.best_estimator_.score(X_train, y_train)\n",
    "test_acc = grid_search.best_estimator_.score(X_test, y_test)\n",
    "print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing with PCA\n",
    "\n",
    "Now that you've fit a baseline classifier, it's time to explore the impacts of using PCA as a preprocessing technique. To start, perform PCA on `X_train`. (Be sure to only fit PCA to `X_train`; you don't want to leak any information from the test set.) Also, don't reduce the number of features quite yet. You'll determine the number of features needed to account for 95% of the overall variance momentarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the explained variance versus the number of features\n",
    "\n",
    "In order to determine the number of features you wish to reduce the dataset to, it is sensible to plot the overall variance accounted for by the first $n$ principal components. Create a graph of the variance explained versus the number of principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,65), pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the number of features to capture 95% of the variance\n",
    "\n",
    "Great! Now determine the number of features needed to capture 95% of the dataset's overall variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "n_over_95 = len(total_explained_variance[total_explained_variance >= .95])\n",
    "n_to_reach_95 = X.shape[1] - n_over_95 + 1\n",
    "print(\"Number features: {}\\tTotal Variance Explained: {}\".format(n_to_reach_95, total_explained_variance[n_to_reach_95-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset the dataset to these principal components which capture 95% of the overall variance\n",
    "\n",
    "Use your knowledge to reproject the dataset into a lower-dimensional space using PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_to_reach_95)\n",
    "X_pca_train = pca.fit_transform(X_train)\n",
    "pca.explained_variance_ratio_.cumsum()[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refit a model on the compressed dataset\n",
    "\n",
    "Now, refit a classification model to the compressed dataset. Be sure to time the required training time, as well as the test and training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_test = pca.transform(X_test)\n",
    "clf = svm.SVC(gamma='auto')\n",
    "%timeit clf.fit(X_pca_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca_acc = clf.score(X_pca_train, y_train)\n",
    "test_pca_acc = clf.score(X_pca_test, y_test)\n",
    "print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(train_pca_acc, test_pca_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "\n",
    "Finally, use grid search to find optimal hyperparameters for the classifier on the reduced dataset. Be sure to record the time required to fit the model, the optimal hyperparameters and the test and train accuracy of the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# ‚è∞ Your code may take several minutes to run\n",
    "clf = svm.SVC()\n",
    "\n",
    "param_grid = {'C' : np.linspace(.1, 10, num=11),\n",
    "             'gamma' : np.linspace(10**-3, 5, num=11)}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "%timeit grid_search.fit(X_pca_train, y_train)\n",
    "train_pca_acc = clf.score(X_pca_train, y_train)\n",
    "test_pca_acc = clf.score(X_pca_test, y_test)\n",
    "print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(train_pca_acc, test_pca_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters \n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the training and test accuracy \n",
    "train_acc = grid_search.best_estimator_.score(X_pca_train, y_train)\n",
    "test_acc = grid_search.best_estimator_.score(X_pca_test, y_test)\n",
    "print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Well done! In this lab, you employed PCA to reduce a high dimensional dataset. With this, you observed the potential cost benefits required to train a model and performance gains of the model itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
